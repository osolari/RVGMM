 \documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
%\usepackage{algorithm}
\usepackage[load-configurations=version-1]{siunitx}
%\usepackage{program}
% Definitions of handy macros can go here

\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage{amsmath,amssymb}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathSymbol{\parallel}{\mathrel}{symbols}{"6B}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}



\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2018}{1-48}{4/00}{10/00}{Omid S. Solari, Yulun Wu, James B. Brown}

% Short headings should be running head and authors last names

\ShortHeadings{RMVE}{Solari, Wu, Brown}
\firstpageno{1}

\begin{document}

\title{Repulsive Mixture Variational Embedding}

\author{\name Omid S. Solari\thanks{equally contributing authors, alphabetical order} \email solari@berkeley.edu \\
       \addr Department of Statistics\\
       University of California, Berkeley
       \AND
       \name Yulun Wu\footnotemark[1] \email yulun\_wu@berkeley.edu\\
       \addr Department of Statistics\\
       University of California, Berkeley
       \AND
       \name James B. Brown \email jbbrown@lbl.gov\\
       \addr Lawrence Berkeley National Laboratory and Department of Statistics\\
       University of California, Berkeley}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file


\end{abstract}

tentative achievements:
\begin{enumerate}
    \item separable and stable latent mixtures
    \item better performance on entangled clustering
    \item cluster single cell data and map them to body
\end{enumerate}


\begin{keywords}
Gaussian Mixtures, Clustering, Variational Auto-Encoders
\end{keywords}

\section{Introduction}

\begin{itemize}
    \item review use cases of vae/gmvae
    \item review uses in genomics specifically
\end{itemize}

\section{Background}


\begin{itemize}
    \item review k-means and gaussian mixture models
    \item review variational auto-encoder
    \item introduce repulsive priors
    \item set up the use for single cell data
\end{itemize}

mention the shortcomings of GMVAE.

\section{Problem Statment}

general definition of Gaussian mixture models


\section{Model}

\subsection{Gaussian Mixture Variational Auto-Encoder}

Dependency structure: (draw a graph)

\subsubsection{Variational Lower Bound}
\begin{equation} \label{ELBO}
\mathcal{L}_\textit{ELBO} = \E_{q_\phi (
    \bm{v}_\textit{latent} | \bm{v}_\textit{observed})} 
    \left[ \log \frac{p_\theta (\bm{v}_\textit{latent}, 
    \bm{v}_\textit{observed})}{ q_\phi (
    \bm{v}_\textit{latent} | \bm{v}_\textit{observed})} 
    \right]
\end{equation}

\subsubsection{Supervised}

TODO: define notations

\begin{equation} \label{supDeriv}
\begin{split}
\mathcal{D} \left[ q_\phi (\bm{z} | \bm{x}, \bm{y}) 
    \parallel p_\theta (\bm{z} | \bm{x}, \bm{y}) 
    \right] 
 & = - \E_{q_\phi (\bm{z} | \bm{x}, \bm{y})} \left[ 
    \log \frac{p_\theta (\bm{x}, \bm{y}, \bm{z})}{
    q_\phi (\bm{z} | \bm{x}, \bm{y})} \right] + \log 
    p_\theta (\bm{x}, \bm{y})\\
 & = - \E_{q_\phi (\bm{z} | \bm{x}, \bm{y})} \left[
    \log p_\theta (\bm{x} | \bm{z}) + 
    \log p_\theta (\bm{z} | \bm{y}) - \log q_\phi (
    \bm{z} | \bm{x}, \bm{y}) \right] \\
 & \qquad - \log \frac{p (\bm{y})}{q_\phi (\bm{y} | 
    \bm{x})} - \log q_\phi (\bm{y} | \bm{x}) + 
    \log p_\theta (\bm{x}, \bm{y})
\end{split}
\end{equation}

\begin{equation} \label{supELBO}
\begin{split}
\mathcal{L}_\textit{ELBO} 
 & = \E_{q_\phi (\bm{z} | 
    \bm{x}, \bm{y})} \left[ \log p_\theta (\bm{x} | 
    \bm{z}) \right] + \log q_\phi (\bm{y} | \bm{x}) \\
 & \qquad - \mathcal{D} \left[ q_\phi (
    \bm{z} | \bm{x}, \bm{y}) \parallel p_\theta (
    \bm{z} | \bm{y}) \right] - \left[ q_\phi (
    \bm{y} | \bm{x}) \parallel p (\bm{y}) \right]
\end{split}
\end{equation}

(Sometimes people add term $\alpha \dot \log q_\phi (
\bm{y} | \bm{x})$ to amplify classification loss.) \\ 

\subsubsection{Unsupervised}

\begin{equation} \label{eq1}
\mathcal{D} \left[ q_\phi (\bm{y}, \bm{z} | \bm{x}) || 
    p_\theta (\bm{y}, \bm{z} | \bm{x}) \right] 
 & = - \E_{q_\phi (\bm{y}, \bm{z} | \bm{x})} \left[ 
    \log \frac{p_\theta (\bm{x}, \bm{y}, \bm{z})}{
    q_\phi (\bm{y}, \bm{z} | \bm{x})} \right] + \log 
    p_\theta (\bm{x})
\end{equation}

\begin{equation} \label{supELBO}
\begin{split}
\mathcal{L}_\textit{ELBO} 
 & = \E_{q_\phi (\bm{z} | \bm{x}, \bm{y})} \left[ \log 
    p_\theta (\bm{x} | \bm{z}) \right] \\
 & \qquad - \E_{q_\phi (\bm{y} | \bm{x})} 
    \mathcal{D} \left[ q_\phi (\bm{z} | \bm{x}, 
    \bm{y}) \parallel p_\theta (\bm{z} | \bm{y}) 
    \right] - \mathcal{D} \left[ q_\phi (\bm{y} | 
    \bm{x}) \parallel p (\bm{y}) \right]
\end{split}
\end{equation}

\subsubsection{Estimation and Propagation}

Sampling estimation for $q_\phi$: TODO \\

\noindent Understanding the back-propagation for unsupervised case:
$\E_{q_\phi (\bm{y} | \bm{x})} 
    \mathcal{D} \left[ q_\phi (\bm{z} | \bm{x}, 
    \bm{y}) \parallel p_\theta (\bm{z} | \bm{y}) 
    \right]$

\begin{lemma}
In sampling estimation, The KL divergence term 
that regularizes latent variable $z$: 
$- \E_{q_\phi (\bm{y} | \bm{x})} 
\mathcal{D} \left[ q_\phi (\bm{z} | \bm{x}, 
\bm{y}) \parallel p_\theta (\bm{z} | \bm{y}) 
\right]$ in our model is a prior-weighted 
K-Means Model on latent prior $p_\theta (z)$.
\end{lemma}

\begin{proof}
\begin{equation} \label{pfKmeans}
\begin{split}
- \E_{q_\phi (\bm{y} | \bm{x})} \mathcal{D} \left[ 
q_\phi (\bm{z} | \bm{x}, \bm{y}) \parallel p_\theta 
(\bm{z} | \bm{y}) \right] 
 & = - \sum_{\bm{y}} q_\phi (\bm{y} | \bm{x}) 
    \cdot \frac{1}{s} \sum_{\bm{z_1} \dots \bm{z_s} 
    \in q_\phi (\bm{z} | \bm{x}, \bm{y})} 
    \log \frac{q_\phi (\bm{z} | \bm{x}, \bm{y})}{
    p_\theta (\bm{z} | \bm{y})} \\
 & = \frac{1}{s} \sum_{\bm{y}} q_\phi (\bm{y} | 
    \bm{x}) \sum_{\bm{z}} \log p (\bm{z} | 
    \mu_\theta(\bm{y}), \bm{I}) - 
    \textit{const}(p_\theta) \\
 & = - \frac{1}{2s} \sum_{\bm{y}} q_\phi (\bm{y} | 
    \bm{x}) \sum_{\bm{z}} {|| \bm{z} - 
    \mu_\theta(\bm{y}) ||}^2 - 
    \textit{const}(p_\theta)
\end{split}
\end{equation}

This is a K-Means model with the learnt likelihood of 
labels $q_\phi (\bm{y} | \bm{x})$ replacing the standard 
weight $1 / |S_{\bm{y}}|$.
\end{proof}

In SGD update, we won't be able 
to track the total number of data in each mixture. 
Alternatively, weighting the squared deviations in this 
fashion makes the learning of label distribution more 
responsible to the latent prior. \\

\begin{remark}
Unsupervised RMVE reduces to VaDE in a simple case.
\end{remark}

When we assume conditional independence $q_\phi(\bm{y}, \bm{z} | \bm(x)) = q_\phi(\bm{y} | 
\bm{x}) q_\phi(\bm{z} | \bm{x})$, and only consider regularizing $\bm{z}$: 
$\mathcal{D} \left[ q_\phi (\bm{z} | \bm{x}) || 
p_\theta (\bm{z} | \bm{x}) \right]$, it reduces to VaDE. \\

\noindent note that the update of prior parameter is only in response to $p_\theta(\bm{z} | \bm{y})$ \\

%\begin{lemma}
%Sampling SGD update is implicitly an MLE.
%\end{lemma}

\subsection{Bayesian Repulsive Prior}

find a nice drawing software for neural nets that allows depicting repulsion!

\begin{lemma}
some lemma or theorem on covariance repulsion! Use HSIC
\end{lemma}

\subsection{Regularization}

\section{Experiments}



\subsection{Simulation}
\label{subsec:sim}

Simple MNIST is sufficient

\subsection{Single Cell Clustering}
\label{subsec:tcga}

Use The Human Cell Atlas data to show that 1. we can cluster them 2. with a simple linear transformation, those cells are mappable to their physical place! A cool picture ensues!
\cite{weinstein2013cancer}

\section{Conclusion}
\label{sec:conclusion}


\newpage

\appendix


\vskip 0.2in
\bibliography{jmlr}

\end{document}